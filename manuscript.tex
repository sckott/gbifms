\documentclass[author-year, review, 11pt]{components/elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}
\usepackage{lineno} % add 
  \linenumbers % turns line numbering on 
\bibliographystyle{elsarticle-harv}
\biboptions{sort&compress} % For natbib
\usepackage{graphicx}
\usepackage{booktabs} % book-quality tables
%% Redefines the elsarticle footer
\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{\it \hfill\today}%
 \let\@evenfoot\@oddfoot}
\makeatother

% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textbf{{#1}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={rgbif: R client for working with GBIF species occurrence data},
            colorlinks=true,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header



\begin{document}
\begin{frontmatter}

  \title{rgbif: R client for working with GBIF species occurrence data}
    \author[cstar]{Scott Chamberlain\corref{c1}}
   \ead{scott(at)ropensci.org} 
   \cortext[c1]{Corresponding author}
      \address[cstar]{University of California, Berkeley, CA, USA}    
  
  \begin{abstract}
  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    xxx
  \item
    xxx
  \item
    xxx
  \item
    xxxx
  \end{enumerate}
  \end{abstract}
  
 \end{frontmatter}


\section{Introduction}\label{introduction}

Perhaps the most fundamental element in many fields of ecology is the
inividual. How many individuals of each species in a given location
forms the basis for many sub-fields of ecology and evolution. Some
research questions necessitate collecting new data, while others can
easily take advantage of existing data. In fact, some ecology fields are
built largely on existing data, e.g., macro-ecology (Brown, 1995; Beck
et al., 2012).

Data on individuals, including which species, and where they're found,
can be used for a large number of research questions. Biodiversity
records have been used for a suite of other use cases: validating
habitat suitability models with real occurrence data (Ficetola et al.,
2014); ancestral range reconstruction (Ferretti et al., 2015; Mar{í}a
Mendoza et al., 2015); development of invasive species watch lists
(Faulkner et al., 2014); evaluate risk of invasive species spread
(Febbraro et al., 2013); and effects of climate change on future
biodiversity (Brown et al., 2015).

In addition to wide utility, this data is important for conservation.
Biodiversity loss is one of the greatest challenges of our time (Pimm et
al., 2014). Some have called this the sixth great mass extinction
(Ceballos et al., 2015). Given this challenge there is a great need for
data on specimen records, whether collected from live sightings in the
field or specimens in museums.

There are many online services that collect and maintain specimen
records. However, Global Biodiversity Information Facility (hereafter,
GBIF, \url{http://www.gbif.org/}) is the largest collection of
biodiversity records globally, currently with 580 million records, 1.6
million taxa, 15,000 datasets from 770 publishers (figures collected on
2015-10-04). Many large biodiversity warehouses such as iNaturalist
(\url{http://www.inaturalist.org/}), VertNet
(\url{http://vertnet.org/}), and USGS's Biodiversity Information Serving
Our Nation (BISON; \url{http://bison.usgs.ornl.gov/}) all feed into
GBIF.

Herein, we describe the \texttt{rgbif} library (Chamberlain et al.) for
working with GBIF data in the R programming environment (R Core Team,
2014). R is a widely used language in academia, and in non-profit and
private sectors. Importantly, R makes it easy to do all of the steps of
the research process, including data management, data manipulation and
cleaning, statistics, and vizualization. Thus, an R client for getting
GBIF data is a powerful tool to facilitate reproducible research.

\section{The rgbif package}\label{the-rgbif-package}

The \texttt{rgbif} package is completely written in R, uses an
\href{http://choosealicense.com/licenses/mit/}{MIT license} to maximize
use everywhere. \texttt{rgbif} is developed publicly on GitHub at
\href{https://github.com/ropensci/rgbif}{\url{https://github.com/ropensci/rgbif}},
where development versions of the package can be installed, and bugs and
feature requests reported. Stable versions of \texttt{rgbif} can be
installed from
\href{https://cran.rstudio.com/web/packages/rgbif/}{CRAN}, the
distribution network for R packages. \texttt{rgbif} is part of the
rOpenSci project, a developer network making R software to facilitate
reproducible research.

\subsection{Package interface}\label{package-interface}

\texttt{rgbif} is designed following the
\href{http://www.gbif.org/developer/summary}{GBIF Application
Programming Interface}, or API. The GBIF API has four major components:
registry, taxonomic names, occurrences, and maps. We ignore maps in
\texttt{rgbif} as it is concerned with generating maps primarily for web
applications. \texttt{rgbif} has a suite of functions dealing with each
of registry, taxonomic names, and occurrences - we'll go through each in
turn describing design and example usage.

\subsection{Registry}\label{registry}

The GBIF reqistry API services are spread across four sets of functions:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Datasets
\item
  Installations
\item
  Networks
\item
  Nodes
\item
  Organizations
\end{itemize}

Datasets are owned by organizations. Organizations are endorsed by nodes
to share datasets with GBIF. Datasets are published through
institutions, which may be hosted at another organization. A network is
a group of datasets (managed by GBIF).

\subsubsection{Datasets}\label{datasets}

Dataset functions include search, dataset metadata retrieval, and
dataset metrics. Searching for datasets is an important part of the
discovery process. One can search for datasets on the GBIF web portal.
However, programmatic searching using this package is much more
powerful. Identifying datasets appropriate for a research question is
helpful as you can get metadata for each dataset, and track down dataset
specific problems, if any.

The \texttt{dataset\_search()} function is one way to search for
datasets. Here, we search for the query term ``oregon'', which finds any
datasets that have terms matching that term.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res <-}\StringTok{ }\KeywordTok{dataset_search}\NormalTok{(}\DataTypeTok{query =} \StringTok{"oregon"}\NormalTok{)}
\NormalTok{res$data$datasetTitle[}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{]}
\CommentTok{#>  [1] "SDNHM Birds Collection"                                                }
\CommentTok{#>  [2] "CM Birds Collection"                                                   }
\CommentTok{#>  [3] "condoncollection"                                                      }
\CommentTok{#>  [4] "Taxonomy in Flux Checklist"                                            }
\CommentTok{#>  [5] "Wool carder bees of the genus Anthidium in the Western Hemisphere"     }
\CommentTok{#>  [6] "Bryophyte Collection - University of Washington Herbarium (WTU)"       }
\CommentTok{#>  [7] "University of British Columbia Herbarium (UBC) - Bryophytes Collection"}
\CommentTok{#>  [8] "UWFC Ichthyology Collection"                                           }
\CommentTok{#>  [9] "Lichen Collection - University of Washington Herbarium (WTU)"          }
\CommentTok{#> [10] "UWBM Mammalogy Collection"}
\end{Highlighting}
\end{Shaded}

Also, check out \texttt{datasets()} and \texttt{dataset\_suggest()} for
searching for datasets.

\paragraph{Dataset metrics}\label{dataset-metrics}

Dataset metrics are another useful way of digging in to figure out what
datasets you may want to use. One drawback is that these data are only
available for datasets of type \emph{checklist}, but there are quite a
lot of them (2474).

Here, we seaerch for dataset metrics for a single dataset, with uuid
\texttt{ec93a739-1681-4b04-b62f-3a687127a17f}, a checklist of the ants
(Hymenoptera: Formicidae) of the World.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res <-}\StringTok{ }\KeywordTok{dataset_metrics}\NormalTok{(}\DataTypeTok{uuid=}\StringTok{'ec93a739-1681-4b04-b62f-3a687127a17f'}\NormalTok{)}
\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{rank =} \KeywordTok{names}\NormalTok{(res$countByRank),}
           \DataTypeTok{count =} \KeywordTok{unname}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(res$countByRank)))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[c]{@{}lr@{}}
\toprule
rank & count\tabularnewline
\midrule
\endhead
SPECIES & 13710\tabularnewline
SUBSPECIES & 3234\tabularnewline
GENUS & 726\tabularnewline
TRIBE & 53\tabularnewline
SUBFAMILY & 20\tabularnewline
FAMILY & 2\tabularnewline
KINGDOM & 1\tabularnewline
PHYLUM & 1\tabularnewline
CLASS & 1\tabularnewline
ORDER & 1\tabularnewline
\bottomrule
\end{longtable}

\subsubsection{Networks, nodes, and
installations}\label{networks-nodes-and-installations}

Networks, nodes and installations are at a higher level of organization
above datasets, but can be useful if you want to explore data from given
organizations. Here, we search for the first 10 GBIF networks, returning
just the title field.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{networks}\NormalTok{(}\DataTypeTok{limit=}\DecValTok{10}\NormalTok{)$data$title}
\CommentTok{#>  [1] "GBIF Backbone Sources"                                      }
\CommentTok{#>  [2] "Canadensys"                                                 }
\CommentTok{#>  [3] "Southwest Collections of Arthropods Network (SCAN)"         }
\CommentTok{#>  [4] "VertNet"                                                    }
\CommentTok{#>  [5] "Dryad"                                                      }
\CommentTok{#>  [6] "GBIF Network"                                               }
\CommentTok{#>  [7] "The Knowledge Network for Biocomplexity (KNB) "             }
\CommentTok{#>  [8] "Online Zoological Collections of Australian Museums (OZCAM)"}
\CommentTok{#>  [9] "Catalogue of Life"                                          }
\CommentTok{#> [10] "Ocean Biogeographic Information System (OBIS)"}
\end{Highlighting}
\end{Shaded}

\subsection{Taxonomic names}\label{taxonomic-names}

The GBIF taxonomic names API services are spread across five functions:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Search GBIF name backbone - \texttt{name\_backbone()}
\item
  Search across all checklists - \texttt{name\_lookup()}
\item
  Quick name lookup - \texttt{name\_suggest()}
\item
  Name usage of a name according to a checklist - \texttt{name\_usage()}
\item
  GBIF name parser - \texttt{parsenames()}
\end{itemize}

The goal of these name functions is often to settle on a taxonomic name
known to GBIF's database. This serves two purposes: 1) when referring to
a taxonomic name, you can point to a URI on the internet, and 2) you can
search for metadata on that species, and occurrences of that species in
GBIF.

Taxonomic names are particularly tricky. Many different organizations
have their own unique codes for the same taxonomic names, and some
taxonomic groups have preferred sources for the definitive names for
that group.

When searching for occurrences (see below) you can search by taxonomic
name (and other filters, e.g., taxonomic rank), but you're probably
better off figuring out the taxonomic key in the GBIF backbone taxonomy,
and using that to search for occurrences. The \texttt{taxonkey}
parameter in the GBIF occurrences API expects a GBIF backbone taxon key.

\subsubsection{GBIF Backbone}\label{gbif-backbone}

The GBIF backbone taxonomy is used in GBIF to have a consistent way to
refer to taxonomic names throughout their services. The backbone has
4410899 unique names and 2497114 species names. The backbone taxonomy is
also a dataset with key \texttt{d7dddbf4-2cf0-4f39-9b2a-bb099caae36c}
(\url{http://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c}).

We can search the backbone taxonomy with the function
\texttt{name\_backbone()}. Here, we're searching for the name
\emph{Poa}, restricting to genera, and the family \emph{Poaceae}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res <-}\StringTok{ }\KeywordTok{name_backbone}\NormalTok{(}\DataTypeTok{name=}\StringTok{'Poa'}\NormalTok{, }\DataTypeTok{rank=}\StringTok{'genus'}\NormalTok{, }\DataTypeTok{family=}\StringTok{'Poaceae'}\NormalTok{)}
\NormalTok{res[}\KeywordTok{c}\NormalTok{(}\StringTok{'usageKey'}\NormalTok{, }\StringTok{'kingdom'}\NormalTok{)]}
\CommentTok{#> $usageKey}
\CommentTok{#> [1] 2704173}
\CommentTok{#> }
\CommentTok{#> $kingdom}
\CommentTok{#> [1] "Plantae"}
\end{Highlighting}
\end{Shaded}

\subsubsection{Name searching}\label{name-searching}

One of the quickest ways to search for names is using
\texttt{name\_suggest()}, which does a very quick search and returns
minimal data. Here, we're searching for the query tem \emph{Pum}, and we
get back a bunch of names:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{name_suggest}\NormalTok{(}\DataTypeTok{q=}\StringTok{'Pum'}\NormalTok{, }\DataTypeTok{limit =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[c]{@{}rll@{}}
\toprule
key & canonicalName & rank\tabularnewline
\midrule
\endhead
3269133 & Pumilus & GENUS\tabularnewline
4407849 & Pumilinura & GENUS\tabularnewline
4323990 & Pumiliopsis & GENUS\tabularnewline
4324083 & Pumiliopes & GENUS\tabularnewline
4161281 & Pumilea & GENUS\tabularnewline
4312370 & Pumilopagurus & GENUS\tabularnewline
\bottomrule
\end{longtable}

\subsection{Occurrences}\label{occurrences}

GBIF provides two ways to get occurrence data: through the
\texttt{/occurrence/search} route (see \texttt{occ\_search}), or via the
\texttt{/occurrence/download} route (many functions, see below).
\texttt{occ\_search()} is the main funtion for the search route, and is
more appropriate for smaller data, while \texttt{occ\_download*()}
functions are more appropriate for larger data requests.

Large is of course a subjective term. When you hit a ``large dataset''
will depend primarily on the size of the your data request. GBIF imposes
for any given search a limit of 200,000 records in the search service,
after which point you can't download any more records for that search.
However, you can download more records for different searches.

We think the search service is still quite useful for many people even
given the 200,000 limit. For those that need more data, we have created
a similar interface in the \texttt{download\_*()} functions, that should
be easy to use. Users should take note that using the download service
has a few extra steps to get data into R, but is straight-forward.

\subsubsection{Download API}\label{download-api}

The download API syntax is similar to the occurrence search API in that
the same parameters are used, but the way in which the query is defined
is different. For example, in the download API you can do greater than
searches (i.e., \texttt{latitude \textgreater{} 50}), whereas you can
not do that in the occurrence search API. Thus, we can't make the query
interace exactly the same for both search and download functions.

Using the download service can be as few as three steps: 1) Request data
via a search; 2) Download data; 3) Import data into R.

Request data download given a query. Here, we search for the taxon key
\texttt{3119195}, which is the key for \emph{Helianthus annuus}
(\url{http://www.gbif.org/species/3119195}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{occ_download}\NormalTok{(}\StringTok{'taxonKey = 3119195'}\NormalTok{)}
\CommentTok{#> <<gbif download>>}
\CommentTok{#>   Username: xxxx}
\CommentTok{#>   E-mail: xxxx}
\CommentTok{#>   Download key: 0000840-150615163101818}
\end{Highlighting}
\end{Shaded}

You can check on when the download is ready using the functions
\texttt{occ\_download\_list()} and \texttt{occ\_download\_meta()}. When
it's ready use \texttt{occ\_download\_get()} to download the dataset to
your computer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(res <-}\StringTok{ }\KeywordTok{occ_download_get}\NormalTok{(}\StringTok{"0000840-150615163101818"}\NormalTok{, }\DataTypeTok{overwrite =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{#> <<gbif downloaded get>>}
\CommentTok{#>   Path: ./0000840-150615163101818.zip}
\CommentTok{#>   File size: 3.19 MB}
\end{Highlighting}
\end{Shaded}

What's printed out above is a very brief summary of what was downloaded,
the path to the file, and its size (in human readable form).

Next, read the data in to R using the function
\texttt{occ\_download\_import()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{)}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{occ_download_import}\NormalTok{(res)}
\NormalTok{dat %>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(gbifID, decimalLatitude, decimalLongitude)}
\CommentTok{#>       gbifID decimalLatitude decimalLongitude}
\CommentTok{#> 1  725767384        61.01005         24.41740}
\CommentTok{#> 2  725767447        59.82923         23.13550}
\CommentTok{#> 3  725767450        60.38505         25.17449}
\CommentTok{#> 4  725767513        68.37648         23.51963}
\CommentTok{#> 5  725767546        67.19203         24.85820}
\CommentTok{#> 6  725767579        60.21607         24.67412}
\CommentTok{#> 7  725767609        66.49260         25.70471}
\CommentTok{#> 8  725767645        61.36634         24.76218}
\CommentTok{#> 9  725767678        62.29174         27.96500}
\CommentTok{#> 10 725767681        60.28615         22.38489}
\CommentTok{#> ..       ...             ...              ...}
\end{Highlighting}
\end{Shaded}

\paragraph{Downloaded data format}\label{downloaded-data-format}

The downloaded dataset from GBIF is actually a Darwin Core Archive
(DwC-A), an internationally recognized biodiversity informatics standard
(\url{http://rs.tdwg.org/dwc/}). The DwC-A downloaded is a compressed
folder with a number of files, including metadata, citations for each of
the datasets included in the download, and the data itself, in separate
files for each dataset as well as one single \texttt{.txt} file. In
\texttt{occ\_download\_import()}, we simply fetch data from the
\texttt{.txt} file. If you want to dig into the metadata, citations,
etc., it is easily accessible from the folder on your computer.

\subsubsection{Search API}\label{search-api}

The search API follows the GBIF API and is broken down into the
following functions:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Get a single numeric count of occurrenes - \texttt{occ\_count()}
\item
  Search for occurrences - \texttt{occ\_search()}
\item
  Get occurrences by occurrence identifier - \texttt{occ\_get()}
\item
  Get occurrence metadata - \texttt{occ\_metadata()}
\end{itemize}

The main search work-horse is \texttt{occ\_search()}. This function
allows very flexible search definitions. In addition, this function does
paging internally, making it such that the user does not have worry
about the 300 records per request limit - but of course we can't go over
the 200,000 maximum limit.

\ldots{}

\paragraph{Cleaning data}\label{cleaning-data}

GBIF provides optional data issues with each occurrence record. These
issues fall into many different pre-defined classes, covering issues
with taxonomic names, geographic data, and more (see
\texttt{occ\_issues\_lookup()} to find out more information on GBIF
issues; and the same data on
\href{http://gbif.github.io/gbif-api/apidocs/org/gbif/api/vocabulary/OccurrenceIssue.html}{GBIF's
development site}).

\texttt{occ\_issues()} provides a way to easily filter data downloaded
via \texttt{occ\_search()} based on GBIF issues.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out <-}\StringTok{ }\KeywordTok{occ_search}\NormalTok{(}\DataTypeTok{issue=}\StringTok{'DEPTH_UNLIKELY'}\NormalTok{, }\DataTypeTok{limit =} \DecValTok{500}\NormalTok{)}
\KeywordTok{NROW}\NormalTok{(out)}
\CommentTok{#> [1] 4}
\NormalTok{out %>%}\StringTok{ }\KeywordTok{occ_issues}\NormalTok{(-cudc) %>%}\StringTok{ }\NormalTok{.$data %>%}\StringTok{ }\NormalTok{NROW}
\CommentTok{#> [1] 2}
\end{Highlighting}
\end{Shaded}

\subsection{Use cases}\label{use-cases}

\subsubsection{A}\label{a}

\subsubsection{B}\label{b}

\subsubsection{C}\label{c}

\section{Conclusions and future
directions}\label{conclusions-and-future-directions}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  pt 1
\item
  pt 2
\item
  pt 3
\item
  pt 4
\end{itemize}

\subsection{Acknowledgements}\label{acknowledgements}

This project was supported in part by the Alfred P Sloan Foundation
(Grant 2013-6-22).

\subsection{Data Accessibility}\label{data-accessibility}

All scripts and data used in this paper can be found in the permanent
data archive Zenodo under the digital object identifier (DOI). This DOI
corresponds to a snapshot of the GitHub repository at
\href{https://github.com/sckott/msrgbif}{github.com/sckott/msrgbif}.
Software can be found at
\href{https://github.com/ropensci/rgbif}{github.com/ropensci/rgbif},
under the open and permissive MIT license.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

Beck J., Ballesteros-Mejia L., Buchmann CM., Dengler J., Fritz SA.,
Gruber B., Hof C., Jansen F., Knapp S., Kreft H., Schneider A-K., Winter
M., Dormann CF. 2012. Whats on the horizon for macroecology?
\emph{Ecography} 35:673--683.

Brown JH. 1995. \emph{Macroecology}. University of Chicago Press.

Brown KA., Parks KE., Bethell CA., Johnson SE., Mulligan M. 2015.
Predicting plant diversity patterns in madagascar: Understanding the
effects of climate and land cover change in a biodiversity hotspot.
\emph{PLOS ONE} 10:e0122721.

Ceballos G., Ehrlich PR., Barnosky AD., Garcia A., Pringle RM., Palmer
TM. 2015. Accelerated modern human-induced species losses: Entering the
sixth mass extinction. \emph{Science Advances} 1:e1400253--e1400253.

Chamberlain S., Ram K., Barve V., Mcglinn D. \emph{Rgbif: Interface to
the global 'biodiversity' information facility 'aPI'}.

Faulkner KT., Robertson MP., Rouget M., Wilson JR. 2014. A simple, rapid
methodology for developing invasive species watch lists.
\emph{Biological Conservation} 179:25--32.

Febbraro MD., Lurz PWW., Genovesi P., Maiorano L., Girardello M.,
Bertolino S. 2013. The use of climatic niches in screening procedures
for introduced species to evaluate risk of spread: A case with the
american eastern grey squirrel. \emph{PLoS ONE} 8:e66559.

Ferretti F., Verd GM., Seret B., {Š}prem JS., Micheli F. 2015. Falling
through the cracks: The fading history of a large iconic predator.
\emph{Fish and Fisheries}:n/a--n/a.

Ficetola GF., Rondinini C., Bonardi A., Baisero D., Padoa-Schioppa E.
2014. Habitat availability for amphibians and extinction threat: A
global analysis. \emph{Diversity and Distributions} 21:302--311.

Mar{í}a Mendoza., Ospina OE., C{á}rdenas-Henao H., Garc{í}a-R JC. 2015.
A likelihood inference of historical biogeography in the world's most
diverse terrestrial vertebrate genus: Diversification of
direct-developing frogs (craugastoridae: Pristimantis) across the
neotropics. \emph{Molecular Phylogenetics and Evolution} 85:50--58.

Pimm SL., Jenkins CN., Abell R., Brooks TM., Gittleman JL., Joppa LN.,
Raven PH., Roberts CM., Sexton JO. 2014. The biodiversity of species and
their rates of extinction, distribution, and protection. \emph{Science}
344:1246752--1246752.

R Core Team. 2014. \emph{R: A language and environment for statistical
computing}. Vienna, Austria: R Foundation for Statistical Computing.

\end{document}


