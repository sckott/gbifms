---
layout: review, 11pt
linenumbers: true
title: "rgbif: R client for working with GBIF species occurrence data"
author:
  - name: Scott Chamberlain
    affiliation: cstar
    email: scott(at)ropensci.org
    footnote: Corresponding author
address:
  - code: cstar
    address: |
      University of California, Berkeley, CA, USA
abstract: |
      1. xxx

      2. xxx

      3. xxx

      4. xxxx

bibliography: components/references.bib
csl: components/peerj.csl
documentclass: components/elsarticle

output:
  pdf_document:
    template: components/elsarticle.latex
    keep_tex: true
    fig_caption: true
---


```{r compile-settings, include=FALSE}
library("methods")
library("knitr")
opts_chunk$set(tidy = FALSE, warning = FALSE, message = FALSE,
               cache = 1, comment = NA, verbose = TRUE)
basename <- gsub(".Rmd", "", knitr:::knit_concord$get('infile'))
opts_chunk$set(fig.path = paste("components/figure/", basename, "-", sep=""),
               cache.path = paste("components/cache/", basename, "/", sep=""))
```

# Introduction

Perhaps the most fundamental element in many fields of ecology is
the inividual. How many individuals of each species in a given
location forms the basis for many sub-fields of ecology and evolution.
Some research questions necessitate collecting new data, while
others can easily take advantage of existing data. In fact, some
ecology fields are built largely on existing data, e.g., macro-ecology
[@Brown_1995; @Beck_2012].

Data on individuals, including which species, and where they're found,
can be used for a large number of research questions. In addition,
the pool of questions we can answer becomes much larger with more
and better data. In addition to wide utility, this data is important
for conservation. Biodiversity loss is one of the greatest challenges
of our time [@Pimm_2014]. Some have called this the sixth great mass
extinction [@Ceballos_2015]. Given this challenge there is a great
need for data on specimen records, whether collected from live sightings
in the field or specimens in museums.

There are many online services that collect and maintain specimen records.
However, Global Biodiversity Information Facility (hereafter, GBIF,
http://www.gbif.org/) is the largest collection of biodiversity
records globally, currently with 580 million records, 1.6 million
taxa, 15,000 datasets from 770 publishers (figures collected on 2015-10-04).
Many large biodiversity warehouses such as iNaturalist, VertNet, and USGS's
BISON all feed into GBIF.

Herein, we describe a library (rgbif [@rgbif]) for working with
GBIF data in the R programming environment [@R]. R is an extremely
widely used language in academia, and in non-profit and private sectors.
Importantly, R makes it easy to do all of the steps of the
research process, including data management, data manipulation
and cleaning, statistics, and vizualization. Thus, an R client
for getting GBIF data is a powerful tool for reproducible research.

```{r echo=FALSE, results='hide', eval=FALSE}
install.packages("rgbif", dependencies=TRUE)
```

```{r echo=FALSE, results='hide'}
library("rgbif")
```

# The rgbif package

The `rgbif` package is completely written in R, uses an [MIT license][mit]
to maximize use everywhere. `rgbif` is developed publicly on GitHub
at [https://github.com/ropensci/rgbif][ghrgbif], where development versions
of the package can be installed, and bugs and feature requests reported.
Stable versions of `rgbif` can be installed from [CRAN][cranrgbif], the
distribution network for R packages. `rgbif` is part of the rOpenSci project,
a developer network making R software to facilitate reproducible research.

## Package interface

`rgbif` is designed following the [GBIF Application Programming Interface][gbifdocs],
or API. The GBIF API has four major components: registry, species names,
occurrence data, and maps. We ignore maps in `rgbif` as it is concerned with
generating maps for web applications. `rgbif` has a suite of functions dealing
with each of registry, species names, and occurrence data - we'll go through
each in turn describing design and example usage.

## Registry

The GBIF reqistry API services are spread across four sets
of functions:

* Datasets
* Installations
* Networks
* Nodes
* Organizations

### Datasets

Search for datasets

```{r}
res <- dataset_search(query = "oregon")
res$data$datasetTitle[1:10]
```

Get dataset metrics

```{r}
res <- dataset_metrics(uuid='66dd0960-2d7d-46ee-a491-87b9adcfe7b1')
df <- data.frame(rank = names(res$countByRank),
                 count = unname(unlist(res$countByRank)))
knitr::kable(df)
```

### Networks, nodes, and installations

Here, we search for the first give GBIF networks, returning just the
key and title fields.

```{r}
networks(limit=10)$data$title
```

## Species

## Occurrences

GBIF provides two ways to get occurrence data: through the
`/occurrence/search` route (see `occ_search`), or via the
`/occurrence/download` route (many functions, see below).
`occ_search()` is the main funtion for the search route,
and is more appropriate for smaller data, while
`occ_download*()` functions are more appropriate for larger
data requests.

Large is of course a subjective term. When you hit a "large dataset"
will depend primarily on the size of the your data request. GBIF imposes
for any given search a limit of 200,000 records in the search service,
after which point you can't download any more records for that search.
However, you can download more records for different searches.

We think the search service is still quite useful for many people
even given the 200,000 limit. For those that need more data, we have
created a similar interface in the `download_*()` functions, that should
be easy to use. Users should take note that using the download service
has a few extra steps to get data into R, but is straight-forward.

### Download API

The download API syntax is similar to the occurrence search API
in that the same parameters are used, but the way in which the query
is defined is different. For example, in the download API you can
do greater than searches (i.e., `latitude > 50`), whereas you can not
do that in the occurrence search API. Thus, we can't make the query
interace exactly the same for both search and download functions.

Using the download service can be as few as three steps: 1) Request data
via a search; 2) Download data; 3) Import data into R.

Request data download given a query. Here, we search for the taxon key
`3119195`, which is the key for _Helianthus annuus_
(http://www.gbif.org/species/3119195).

```{r}
occ_download('taxonKey = 3119195')
```

You can check on when the download is ready using the functions
`occ_download_list()` and `occ_download_meta()`. When it's ready
use `occ_download_get()` to download the dataset to your computer.

```{r}
(res <- occ_download_get("0000840-150615163101818", overwrite = TRUE))
```

What's printed out above is a very brief summary of what was downloaded,
the path to the file, and its size (in human readable form).

Next, read the data in to R using the function `occ_download_import()`.

```{r}
library("dplyr")
dat <- occ_download_import(res)
dat %>%
  select(gbifID, decimalLatitude, decimalLongitude)
```

#### Downloaded data format

The downloaded dataset from GBIF is actually a Darwin Core Archive (DwC-A), an
internationally recognized biodiversity informatics standard
(http://rs.tdwg.org/dwc/). The DwC-A downloaded is a compressed folder with
a number of files, including metadata, citations for each of the datasets included
in the download, and the data itself, in separate files for each dataset as well
as one single `.txt` file. In `occ_download_import()`, we simply fetch data from the
`.txt` file. If you want to dig into the metadata, citations, etc., it is easily
accessible from the folder on your computer.

### Search API

The search API follows the GBIF API and is broken down into the following functions:

* `occ_count()`
* `occ_search()`
* `occ_get()`
* `occ_metadata()`

The main search work-horse is `occ_search()`. This function allows very flexible
search definitions. In addition, this function does paging internally, making it
such that the user does not have worry about the 300 records per request limit -
but of course we can't go over the 200,000 maximum limit.

...

#### Cleaning data

GBIF provides optional data issues with each occurrence record. These issues fall
into many different pre-defined classes, covering issues with taxonomic names,
geographic data, and more (see `occ_issues_lookup()` to find out more information
on GBIF issues; and the same data on [GBIF's development site][gbifissues]).

`occ_issues()` provides a way to easily filter data downloaded via `occ_search()`
based on GBIF issues.

```{r}
out <- occ_search(issue='DEPTH_UNLIKELY', limit = 500)
NROW(out)
out %>% occ_issues(-cudc) %>% .$data %>% NROW
```

## Use cases

...

# Conclusions and future directions

## Acknowledgements

This project was supported in part by the Alfred P Sloan
Foundation (Grant 2013-6-22).

## Data Accessibility

All scripts and data used in this paper can be found in the permanent
data archive Zenodo under the digital object identifier (DOI). This DOI corresponds to a snapshot of the GitHub repository at [github.com/sckott/msrgbif](https://github.com/sckott/msrgbif). Software
can be found at [github.com/ropensci/rgbif](https://github.com/ropensci/rgbif), under the
open and permissive MIT license.

# References

[mit]: http://choosealicense.com/licenses/mit/
[ghrgbif]: https://github.com/ropensci/rgbif
[cranrgbif]: https://cran.rstudio.com/web/packages/rgbif/
[gbifdocs]: http://www.gbif.org/developer/summary
[gbifissues]: http://gbif.github.io/gbif-api/apidocs/org/gbif/api/vocabulary/OccurrenceIssue.html
