---
layout: review, 11pt
linenumbers: true
title: "R Python, and Ruby clients for GBIF species occurrence data"
author:
  - name: Scott Chamberlain
    affiliation: cstar
    email: scott(at)ropensci.org
    footnote: Corresponding author
  - name: Carl Boettiger
    affiliation: boettig
    email: carl(at)ropensci.org
address:
  - code: cstar
    address: |
      rOpenSci, Museum of Paleontology, University of California, Berkeley, CA, USA
  - code: boettig
    address: |
      rOpenSci, Department of Enivornmental Science, Policy and Management, University of California, Berkeley, CA, USA
abstract: |
      Corresponding Author:

      Scott Chamberlain

      rOpenSci, Museum of Paleontology, University of California, Berkeley, CA, USA

      Email address: [scott@ropensci.org](mailto:scott@ropensci.org)

      \newpage

      Background. The number of individuals of each species in
        a given location forms the basis for many sub-fields of ecology and
        evolution. Data on individuals, including which species, and
        where they're found can be used for a large number of research
        questions. Global Biodiversity Information Facility (hereafter, GBIF)
        is the largest of these. Programmatic clients for GBIF would
        make research dealing with GBIF data much easier and more reproducible.

      Methods. We have developed clients to access GBIF data for each
        of the R, Python, and Ruby programming languages: `rgbif`, `pygbif`,
        `gbifrb`.

      Results. For all clients we describe their design and utility,
        and demonstrate some use cases.

      Discussion. Programmatic access to GBIF will facilitate more open
        and reproducible science - the three GBIF clients described herein
        are a significant contribution towards this goal.

bibliography: components/references.bib
csl: components/peerj.csl
documentclass: components/elsarticle

output:
  pdf_document:
    template: components/elsarticle.latex
    keep_tex: true
    fig_caption: true
---


```{r compile-settings, include=FALSE}
library("methods")
library("knitr")
opts_chunk$set(
  tidy = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = 1,
  comment = '#>',
  collapse = TRUE,
  verbose = TRUE
)

basename <- gsub(".Rmd", "", knitr:::knit_concord$get('infile'))
opts_chunk$set(fig.path = paste("components/figure/", basename, "-", sep=""),
               cache.path = paste("components/cache/", basename, "/", sep=""))

# tibble options
options(tibble.max_extra_cols = 10)
```

\newpage

# Introduction

Perhaps the most fundamental element in many fields of ecology is
the individual organism. The number of individuals of each species in
a given location forms the basis for many sub-fields of ecology and
evolution. Some research questions necessitate collecting new data, while
others can easily take advantage of existing data. In fact, some
ecology fields are built largely on existing data, e.g., macro-ecology
[@Brown_1995; @Beck_2012].

Data on individuals, including which species, and where they're found,
can be used for a large number of research questions. Biodiversity
records have been used for a suite of other use cases:
validating habitat suitability models with real occurrence data
[@Ficetola_2014]; ancestral range reconstruction [@Ferretti_2015;
@Mendoza_2015]; development of invasive species watch lists
[@Faulkner_2014]; evaluating risk of invasive species spread
[@Di_Febbraro_2013]; and effects of climate change on future
biodiversity [@Brown_2015].

In addition to wide utility, this data is important for conservation.
Biodiversity loss is one of the greatest challenges of our time
[@Pimm_2014], and some have called this the sixth great mass
extinction [@Ceballos_2015]. Given this challenge there is a great
need for data on specimen records, whether collected from live sightings
in the field or specimens in museums.

# Global Biodiversity Information Facility

There are many online services that collect and maintain specimen records.
However, Global Biodiversity Information Facility (hereafter, GBIF,
<http://www.gbif.org>) is the largest collection of biodiversity records
globally, currently with 820 million records, roughly 5.9 million
taxa, 36,000 datasets from 1,300 publishers (as of 2016-02-09).
Many large biodiversity warehouses such as iNaturalist
(<http://www.inaturalist.org>), VertNet (<http://vertnet.org>), and USGS's
Biodiversity Information Serving Our Nation (BISON; <http://bison.usgs.ornl.gov>)
all feed into GBIF.

The most important organizational level in GBIF occurrence data is the occurrence
record. The fields in a record vary, but include information about
taxonomy (kingdom, phylum, genus, species names) and their identifiers,
dataset metadata, and locality information including geospatial position.
Going upstream, each record is part of a dataset, where each dataset is
submitted by an organization, organizations are organized into
nodes, datasets are published through institutions (which may be hosted at another
organization), and a network is a group of datasets (managed by GBIF).

Each occurrence record has some taxonomic name associated with it, which itself is
linked to a lot of other taxonomic data - including a master taxonomic backbone
that integrates taxonomies across many taxonomic authorities.

The organization of GBIF matters because you can navigate GBIF data through
these hierarchical organizational levels - it helps to be familiar with the
terminology and how each group relates to another.

# The clients

Although we discuss libraries for R, Python, and Ruby here, we focus mostly on
the R library `rgbif` as it has seen the most developer and user attention,
and is the most mature.

## rgbif

Herein, we describe the `rgbif` software package [@rgbif] for working with
GBIF data in the R programming environment [@R]. R is a widely used
language in academia, as well as non-profit and private sectors.
Importantly, R makes it easy to execute all steps of the
research process, including data management, data manipulation
and cleaning, statistics, and visualization. Thus, an R client
for getting GBIF data is a powerful tool to facilitate
reproducible research.

```{r echo=FALSE, results='hide', eval=FALSE}
install.packages("rgbif", dependencies=TRUE)
```

```{r echo=FALSE, results='hide'}
library("rgbif")
```

The `rgbif` package is nearly completely written in R (a small Javascript
library is included for reading well known text [@wkt]), uses an [MIT license][mit]
to maximize use everywhere. `rgbif` is developed publicly on GitHub
at [https://github.com/ropensci/rgbif][ghrgbif], where development versions
of the package can be installed, and bugs and feature requests reported.
Stable versions of `rgbif` can be installed from [CRAN][cranrgbif], the
distribution network for R packages. `rgbif` is part of the rOpenSci project
(http://ropensci.org), a developer network making R software to facilitate
reproducible research.

## pygbif

`pygbif` [@pygbif] is a Python library for working with GBIF data in
the Python programming environment. Python is a general
purpose programming language used widely in all sectors, and for
all parts of software development including server and client
side use cases. Python is used exclusively in some scientific
disciplines (e.g., astronomy), and has partial usage in other
disciplines. A Python client for GBIF data is an important tool
given the even wider usage of Python than R, though maybe slightly
less than R for ecology/biology disciplines.

```{python eval=FALSE}
pip install pygbif
```

```{python eval=FALSE}
import pygbif
```

The `pygbif` library is less mature and complete than the R package. It also
uses an [MIT license][mit] to maximize use everywhere. `pygbif` is developed
publicly on GitHub at <https://github.com/sckott/pygbif>, where
development versions of the package can be installed, and bugs and feature
requests reported. Stable versions of `pygbif` can be installed from
[pypi][pypipygbif], the distribution network for Python libraries.

## gbifrb

`gbifrb` [@gbifrb] is a library for working with GBIF data in
the Ruby programming environment. Like Python, Ruby is a general
purpose programming language used widely in all sectors. Unlike
Python, Ruby is not used extensively in scientific disciplines.
However, a Ruby client for GBIF data can be an important tool
given how widely Ruby is used for web and web service
development.

```{ruby eval=FALSE}
gem install gbifrb
```

```{ruby eval=FALSE}
require 'gbifrb'
```

The `gbifrb` library is less mature and complete than the R and Python libraries.
It also uses an [MIT license][mit] to maximize use everywhere. `gbifrb` is developed
publicly on GitHub at <https://github.com/sckott/gbifrb>, where
development versions of the package can be installed, and bugs and feature
requests reported. Stable versions of `gbifrb` can be installed from
[Rubygems][gemgbif], the distribution network for Ruby libraries.

### Library interfaces

`rgbif`, `pygbif`, and `gbifrb` are designed following the [GBIF Application Programming Interface][gbifdocs],
or API. The GBIF API has four major components: registry, taxonomic names,
occurrences, and maps. We also include functions to interface with the OAI-PMH
GBIF service; only dataset (registry) information is available via this service, however.
An interface to the GBIF maps API is in development for `rgbif`, but is non-existent for
both `pygbif` and `gbifrb`. All three libraries have a suite of functions dealing with
each of registry, taxonomic, names, and occurrences - we'll go through each in turn
describing design of the user interface and example usage.

## GBIF headers

With each request `rgbif`, `pygbif`, `gbifrb` make to GBIF's API, we send request
headers that tell GBIF what library the request is coming from, including what version of
the library. This helps GBIF know what proportion of requests are coming from
which library, and therefore from R vs. Python vs. Ruby; this information is helpful
for GBIF in thinking about how people are using GBIF data.

## Registry

The GBIF registry API services are spread across five sets of functions
via the main GBIF API:

* Datasets
* Installations
* Networks
* Nodes
* Organizations

Dataset information in general is available via the OAI-PMH service,
functions in `rgbif` prefixed with `gbif_oai_`, but not available in `pygbif`
or `gbifrb` yet.

Datasets are owned by organizations. Organizations are endorsed by nodes to share
datasets with GBIF. Datasets are published through institutions, which may be
hosted at another organization. A network is a group of datasets (managed by GBIF).
Datasets are the units that matter the most with respect to registry information,
while installations, networks, nodes, and organizations are simply higher level
organizational structure.

### Datasets

Dataset functions include search, dataset metadata retrieval, and dataset metrics.
Searching for datasets is an important part of the discovery process. One can
search for datasets on the GBIF web portal. However, programmatic searching using
any of these libraries is more powerful. Identifying datasets appropriate for a
research question is helpful as you can get metadata for each dataset, and track
down dataset specific problems, if any.

The `dataset_search()` function in `rgbif` is one way to search for datasets. Here,
we search for the term "oregon", which finds any datasets that have words
matching that term.

```{r}
res <- dataset_search(query = "oregon")
res$data$datasetTitle[1:10]
```

See also `datasets()` and `dataset_suggest()` in `rgbif` for searching for datasets.

In Python, we can similarly search for datasets. Here, search for datasets of type
`OCCURRENCE`:

```{python eval=FALSE}
from pygbif import registry
registry.datasets(type="OCCURRENCE")
```

In Ruby, we can do the same. Here, search for datasets of type `OCCURRENCE`:

```{ruby eval=FALSE}
require 'gbifrb'
registry = Gbif::Registry
registry.datasets(type: "OCCURRENCE")
```

#### Dataset metrics

Dataset metrics are another useful way of figuring out what
datasets you may want to use. One drawback is that these metrics data are only
available for datasets of type _checklist_, but there are quite a lot of
them (`r dataset_search(type = "checklist")$meta$count`).

Here, in R we search for dataset metrics for a single dataset, with uuid
`ec93a739-1681-4b04-b62f-3a687127a17f`, a checklist of the ants (Hymenoptera:
Formicidae) of the World.

```{r eval=FALSE}
res <- dataset_metrics(uuid='ec93a739-1681-4b04-b62f-3a687127a17f')
data.frame(rank = names(res$countByRank),
           count = unname(unlist(res$countByRank)))
```

```{r echo=FALSE}
res <- dataset_metrics(uuid='ec93a739-1681-4b04-b62f-3a687127a17f')
df <- data.frame(rank = names(res$countByRank),
                 count = unname(unlist(res$countByRank)))
knitr::kable(df)
```

And in Python, get metrics for the same dataset as above:

```{python eval=FALSE}
from pygbif import registry
registry.dataset_metrics(uuid='ec93a739-1681-4b04-b62f-3a687127a17f')
```

The same in Ruby:

```{ruby eval=FALSE}
require 'gbifrb'
registry = Gbif::Registry
registry.dataset_metrics(uuid: 'ec93a739-1681-4b04-b62f-3a687127a17f')
```

### Networks, nodes, and installations

Networks, nodes and installations are at a higher level of organization
above datasets, but can be useful if you want to explore data from given
organizations. Here, in R we search for the first 10 GBIF networks, returning
just the title field.

```{r}
networks(limit = 10)$data$title
```

And in Python:

```{python eval=FALSE}
from pygbif import registry
registry.networks(limit = 10)
```

And in Ruby:

```{ruby eval=FALSE}
require 'gbifrb'
registry = Gbif::Registry
registry.networks(limit: 10)
```

## Taxonomic names

The GBIF taxonomic names API services are spread across five functions in `rgbif`:

* Search GBIF name backbone - `name_backbone()`
* Search across all checklists - `name_lookup()`
* Quick name lookup - `name_suggest()`
* Name usage of a name according to a checklist - `name_usage()`
* GBIF name parser - `parsenames()`

`pygbif` and `gbifrb` have all the same functions, except the name parser goes by
`name_parser()` in `pygbif` and `gbifrb`.

The goal of these name functions is often to settle on a taxonomic name
known to GBIF's database. This serves two purposes: 1) when referring to
a taxonomic name, you can point to a URI on the Internet, and 2) you can
search for metadata on a taxon, and occurrences of that taxon in
GBIF.

Taxonomic names are particularly tricky. Many different organizations have
their own unique codes for the same taxonomic names, and some taxonomic
groups have preferred sources for the definitive names for that group. That's
why it's best to determine what name GBIF uses, and its associated identifier,
for the taxon of interest instead of simply searching for occurrences with a
taxonomic name.

When searching for occurrences (see below) you can search by taxonomic
name (and other filters, e.g., taxonomic rank), but you're probably better off
figuring out the taxonomic key in the GBIF backbone taxonomy, and using
that to search for occurrences. The `taxonkey` parameter in the GBIF occurrences
API expects a GBIF backbone taxon key.

### GBIF Backbone

The GBIF backbone taxonomy is used in GBIF to have a consistent way to refer
to taxonomic names throughout their services. The backbone has `r dataset_metrics("d7dddbf4-2cf0-4f39-9b2a-bb099caae36c")$distinctNamesCount` unique
names and `r dataset_metrics("d7dddbf4-2cf0-4f39-9b2a-bb099caae36c")$countByRank$SPECIES`
species names. The backbone taxonomy is also a dataset with key
`d7dddbf4-2cf0-4f39-9b2a-bb099caae36c`
(<https://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c>).

We can search the backbone taxonomy with the function `name_backbone()` in all
thee clients. Here, we're searching for the name _Poa_, restricting to
genera, and the family _Poaceae_, in R

```{r}
res <- name_backbone(name='Poa', rank='genus', family='Poaceae')
res[c('usageKey', 'kingdom')]
```

And in Python

```{python eval=FALSE}
from pygbif import species
res = species.name_backbone(name='Poa', rank='genus', family='Poaceae')
[ res[x] for x in ['usageKey', 'kingdom'] ]
```

And in Ruby

```{ruby eval=FALSE}
require 'gbifrb'
species = Gbif::Species
res = species.name_backbone(name: 'Poa', rank: 'genus', family: 'Poaceae')
res.select { |k,v| k.match(/usageKey|kingdom/) }
```

### Name searching

One of the quickest ways to search for names is using `name_suggest()`, which
does a very quick search and returns minimal data. Here, we're searching for
the query term _Pum_, and we get back many names:

```{r eval=FALSE}
name_suggest(q='Pum', limit = 6)
```

```{r echo=FALSE}
df <- name_suggest(q='Pum', limit = 6)
knitr::kable(df)
```

The same in Python

```{python eval=FALSE}
from pygbif import species
species.name_suggest(q='Pum', limit = 6)
```

And in Ruby

```{ruby eval=FALSE}
require 'gbifrb'
species = Gbif::Species
species.name_suggest(q: 'Pum', limit: 6)
```

With these results, you can then proceed to search for occurrences with the
taxon key(s), or drill down further with other name searching functions to
get the exact taxon of interest.

## Occurrences

GBIF provides two ways to get occurrence data: through the
`/occurrence/search` route (see `occ_search` in `rgbif`,
`occurrences.search` in `pygbif`, `Occurrences.search` in gbifrb), or via the
`/occurrence/download` route (many functions, see below).

`occ_search()`/`occurrences.search`/`Occurrences.search` are the main functions
for the search route, and are more appropriate when you want less data, while
the download functions are more appropriate for larger data requests.

Small vs. large amounts of data of course is all relative. GBIF imposes for any
given search a limit of 200,000 records in the search
service, after which point you can't download any more records for that
search. However, you can download more records for different searches.

We think the search service is still quite useful for many people
even given the 200,000 limit. For those that need more data, we have
created a similar interface in the download functions that should
be easy to use with minimal work. Users should take note that using the
download service has a few extra steps to get data into R, but is
straight-forward.

The download service, like the occurrence search service, is rate-limited.
That is, you can only have one to three downloads running simultaneously for
your user credentials. However, simply check when a download job is complete,
then you can start a new download request. See "Queuing Download Requests"
below for help automating many download requests in R.

### Download API

The download API syntax is similar to the occurrence search API
in that the same parameters are used, but the way in which the query
is defined is different. For example, in the download API you can
do greater than searches (i.e., `latitude > 50`), whereas you cannot
do that in the occurrence search API. Thus, unfortunately, we couldn't
make the query interface exactly the same for both search and
download functions.

Using the download service can consist of as few as three steps: 1) Request
data via a search; 2) Download data; 3) Import data into R.

Request data download given a query. Here, we search for the taxon key
`3119195`, which is the key for _Helianthus annuus_
(http://www.gbif.org/species/3119195).

```{r eval = FALSE}
occ_download('taxonKey = 3119195')
#> <<gbif download>>
#>   Username: xxxx
#>   E-mail: xxxx
#>   Download key: 0000840-150615163101818
```

You can check on when the download is ready using the functions
`occ_download_list()` and `occ_download_meta()`. When it's ready
use `occ_download_get()` to download the dataset to your computer.

```{r}
(res <- occ_download_get("0000840-150615163101818", overwrite = TRUE))
```

What's printed out above is a very brief summary of what was downloaded,
the path to the file, and its size (in human readable form).

Next, read the data in to R using the function `occ_download_import()`.

```{r eval=FALSE}
library("dplyr")
dat <- occ_download_import(res)
dat %>%
  select(gbifID, decimalLatitude, decimalLongitude)
#>       gbifID abstract accessRights accrualMethod accrualPeriodicity accrualPolicy alternative audience
#> 1  725767384       NA                         NA                 NA            NA          NA       NA
#> 2  725767447       NA                         NA                 NA            NA          NA       NA
#> 3  725767450       NA                         NA                 NA            NA          NA       NA
#> 4  725767513       NA                         NA                 NA            NA          NA       NA
#> 5  725767546       NA                         NA                 NA            NA          NA       NA
#> 6  725767579       NA                         NA                 NA            NA          NA       NA
#> 7  725767609       NA                         NA                 NA            NA          NA       NA
#> 8  725767645       NA                         NA                 NA            NA          NA       NA
#> 9  725767678       NA                         NA                 NA            NA          NA       NA
#> 10 725767681       NA                         NA                 NA            NA          NA       NA
#> ..       ...      ...          ...           ...                ...           ...         ...      ...
#> Variables not shown: available (lgl), bibliographicCitation (chr), conformsTo (lgl), contributor (lgl),
#>      coverage (lgl), created (chr), creator (lgl), date (lgl), dateAccepted (lgl), dateCopyrighted
#>      (lgl), dateSubmitted (lgl), description (lgl), educationLevel (lgl), extent (lgl), format (lgl),
#>      hasFormat (lgl), hasPart (lgl), hasVersion (lgl), identifier (chr), instructionalMethod (lgl),
```

In Python

```{python eval = FALSE}
from pygbif import occurrences as occ
occ.download('taxonKey = 3119195')
(res = occ.download_get("0000840-150615163101818", overwrite = True))
```

We don't have `pygbif` functionality at the moment for importing data, but
it's coming soon.

The Ruby library `gbifrb` does not yet have occurrence download functionality.


#### Downloaded data format

The downloaded dataset from GBIF is a Darwin Core Archive (DwC-A), an
internationally recognized biodiversity informatics standard
([http://rs.tdwg.org/dwc/](http://rs.tdwg.org/dwc/)). The DwC-A downloaded is a
compressed folder with a number of files, including metadata, citations for each
of the datasets included in the download, and the data itself, in separate files for
each dataset as well as one single `.txt` file. In `rgbif::occ_download_import()`, we
simply fetch data from the `.txt` file. If you want to dig into the metadata,
citations, etc., it is easily accessible from the folder on your computer.


### Search API

The search API follows the GBIF API and is broken down into the following functions:

* Get a single numeric count of occurrences - `rgbif`: `occ_count()` / `pygbif`: `occurrences.count` / `gbifrb`: `Occurrences.count`
* Search for occurrences - `rgbif`: `occ_search()` / `pygbif`: `occurrences.search` / `gbifrb`: `Occurrences.search`
* A simplified and optimized version of `rgbif`: `occ_search()` or `occ_data()` / none / none
* Get occurrences by occurrence identifier - `rgbif`: `occ_get()` / `pygbif`: `occurrences.get` / `gbifrb`: `Occurrences.get`
* Get occurrence metadata - `rgbif`: `occ_metadata()` / `pygbif`: various / `gbifrb`: various


#### Search for occurrences

The main search work-horse is `occ_search()`. This function allows very flexible
search definitions. In addition, this function does paging internally, making it
such that the user does not have worry about the 300 records per request limit -
but of course we can't go over the 200,000 maximum limit.

The output of `occ_search()` presents a compact `data.frame` so that no matter
how large the `data.frame`, the output is easily assessed because only a few of
the records (rows) are shown, only a few columns are shown (with others shown in
name only), and metadata is shown on top of the `data.frame` to indicate data
found and returned, media records found, unique taxonomic hierarchies returned,
and the query executed.

The output of these examples, except one, aren't shown.

Search by species name, using `name_backbone()` first to get key

__R__

```{r}
library(rgbif)
(key <- name_suggest(q = 'Helianthus annuus', rank = 'species')$key[1])
occ_search(taxonKey = key, limit = 2)
```

__Python__

```{python eval=FALSE}
from pygbif import species
from pygbif import occurrences as occ
key = species.name_suggest(q = 'Helianthus annuus', rank = 'species')['data'][0]['key']
occ.search(taxonKey = key, limit = 2)
```

__Ruby__

```{ruby eval=FALSE}
require 'gbifrb'
species = Gbif::Species
occ = Gbif::Occurrences
key = species.name_suggest(q: 'Helianthus annuus', rank: 'species')['data'][0]['key']
occ.search(taxonKey: key, limit: 2)
```

Instead of getting a taxon key first, you can search for a name directly

__R__


```{r eval=FALSE}
occ_search(scientificName = 'Ursus americanus')
```

__Python__


```{python eval=FALSE}
occ.search(scientificName = 'Ursus americanus')
```

__Ruby__

```{ruby eval=FALSE}
occ.search(scientificName: 'Ursus americanus')
```

Search for many species

__R__

```{r eval=FALSE}
splist <- c('Cyanocitta stelleri', 'Junco hyemalis', 'Aix sponsa')
keys <- sapply(splist, function(x) name_suggest(x)$key[1], USE.NAMES = FALSE)
occ_search(taxonKey = keys, limit = 5, return = 'data')
```

__Python__

```{python eval=FALSE}
from pygbif import species
from pygbif import occurrences as occ
splist = ['Cyanocitta stelleri', 'Junco hyemalis', 'Aix sponsa']
keys = [ species.name_suggest(x)['data'][0]['key'] for x in splist ]
occ.search(taxonKey = keys, limit = 5)
```

__Ruby__

```{ruby eval=FALSE}
species = Gbif::Species
occ = Gbif::Occurrences
splist = ['Cyanocitta stelleri', 'Junco hyemalis', 'Aix sponsa']
keys = [ species.name_suggest(x)['data'][0]['key'] for x in splist ]
occ.search(taxonKey: keys, limit: 5)
```

Spatial search, based on well known text format [@wkt], or a bounding box set of
four coordinates. The well known text string and the bounding box in the below
example specify the same rectangular area in California, centering
approximately on Sacramento. Whereas the bounding box format requires
`longitude SW corner, latitude SW corner, longitude NE corner, latitude NE corner`,
the well known text string requires an extra long/lat pair to close the polygon.

__R__

```{r eval=FALSE}
# well known text
wkt <- 'POLYGON((-122.6 39.9,-120.0 39.9,-120.0 37.9,-122.6 37.9,-122.6 39.9))'
occ_search(geometry = wkt, limit = 20)
# bounding box
occ_search(geometry = c(-122.6,37.9,-120.0,39.9), limit = 20)
```

__Python__

```{python eval=FALSE}
from pygbif import occurrences as occ
# well known text
occ.search(geometry = 'POLYGON((30.1 10.1, 10 20, 20 40, 40 40, 30.1 10.1))', limit = 20)
# bounding box
occ.search(geometry = '-125.0,38.4,-121.8,40.9', limit = 20)
```

__Ruby__

```{ruby eval=FALSE}
occ = Gbif::Occurrences
# well known text
occ.search(geometry: 'POLYGON((30.1 10.1, 10 20, 20 40, 40 40, 30.1 10.1))', limit: 20)
# bounding box
occ.search(geometry: '-125.0,38.4,-121.8,40.9', limit: 20)
```

Get only occurrences with lat/long data using the `hasCoordinate` parameter

__R__

```{r eval=FALSE}
occ_search(hasCoordinate = TRUE, limit = 5)
```

__Python__

```{python eval=FALSE}
from pygbif import occurrences as occ
occ.search(hasCoordinate = True, limit = 5)
```

__Ruby__

```{ruby eval=FALSE}
occ = Gbif::Occurrences
occ.search(hasCoordinate: true, limit: 5)
```

Get only those occurrences with spatial issues. Spatial issues are a set of issues
that are returned in the `issues` field. They each indicate something different
about that record. For example, the issue `COUNTRY_COORDINATE_MISMATCH` indicates
that the interpreted occurrence coordinates fall outside of the indicated country.
You can see how that might be useful when it comes to cleaning your data prior
to analysis/visualization.

__R__

```{r eval=FALSE}
occ_search(hasGeospatialIssue = TRUE, limit = 5)
```

__Python__

```{python eval=FALSE}
from pygbif import occurrences as occ
occ.search(hasGeospatialIssue = True, limit = 5)
```

__Ruby__


```{ruby eval=FALSE}
occ = Gbif::Occurrences
occ.search(hasGeospatialIssue: true, limit: 5)
```




#### Data cleaning

GBIF provides optional data issues with each occurrence record. These issues fall
into many different pre-defined classes, covering issues with taxonomic names,
geographic data, and more (see `rgbif::occ_issues_lookup()` to find out more information
on GBIF issues; and the same data on [GBIF's development site][gbifissues]).

`rgbif::occ_issues()` provides a way to easily filter data downloaded via `rgbif::occ_search()`
based on GBIF issues.

```{r}
out <- occ_search(issue = 'DEPTH_UNLIKELY', limit = 500)
NROW(out)
out %>% occ_issues(-cudc) %>% .$data %>% NROW
```

There's no equivalent interface in `pygbif` or `gbifrb` yet.

## Mapping

An obvious downstream use case for species occurrence data is to map the
data. `rgbif` per se is largely not concerned with making this easier,
although we do have a simple wrapper around `ggplot2` to make it easy to
get a quick plot of occurrence data. For example, here we plot 100
occurrences for _Puma concolor_.

```{r fig.width=8, fig.height=4}
key <- name_backbone(name='Puma concolor')$speciesKey
dat <- occ_search(taxonKey = key, limit = 100, hasCoordinate = TRUE)
gbifmap(dat$data)
```

Another package, [mapr][mapr], is the perfect mapping companion to `rgbif`.
It has convenient functions for handling input data from `rgbif`, `spocc`,
or arbitrary data.frame's, and output plots for base plots, `ggplot2`,
`ggmap` (`ggplot2` with map layers underneath), and interactive maps on
GitHub gists or with Leaflet.js.

There's no equivalent interface in `pygbif` or `gbifrb`.



## GBIF data in other R packages

We discuss usage of GBIF data in other R packages throughout the
manuscript, but provide a synopsis here for clarity.

### taxize

Some of the GBIF taxonomic services are also available in [taxize][taxize],
an R package that focuses on getting data from taxonomic data sources on
the web. For example, with `get_gbifid()` one can get GBIF IDs used for
a set of taxonomic names - then use those IDs in other functions in `taxize`
to get additional information, like taxonomically downstream children.

### spocc

GBIF occurrence data is available in the R package [spocc][spocc] via `rgbif`.
`spocc` is a unified interface for fetching species occurrence data from
many sources on the web. For example, a user can collect occurrence data
from GBIF, iDigBio, and iNaturalist, and easily combine them, then use other
packages to clean and visualize the data.



# R vs. Python vs. Ruby

Both R and Python are commonly used in science, and can be used for similar
tasks. Python, however, is a more general programming language, and can be
used in more contexts than R can be used in. Ruby is used very little in
science; but, like Python, Ruby is very widely used as a general purpose
programming language, with heavy use in web development and web services.

The three clients can do a lot of the same tasks. We envision `rgbif`
being more common in workflows of academics asking research questions,
whereas `pygbif` and `gbifrb` can do that as well, but may be more easily
used in a website.

The R client `rgbif` has had much more development time than `pygbif`
and `gbifrb`, but with time `pygbif` and `gbifrb` will become equally
mature.



# Use cases

The following are three use cases for the R library `rgbif`: niche modeling,
spatial change in biodiversity, and distribution mapping.

### Ecological niche modeling

```{r niche-modeling, child="use-cases/niche-modelling.Rmd"}
```

### Biodiversity in big cities

```{r city-diversity, child="use-cases/city-diversity.Rmd"}
```

### Valley oak occurrence data comparison

```{r oak-occurrences, child="use-cases/oak-occurrences.Rmd"}
```

# Conclusions and future directions

The `rgbif`, `pygbif`, and `gbibrb` libraries provide programmatic interfaces
to GBIF's application programming interface (API) - a powerful tool for
working with species occurrence data, and facilitating reproducible research.
In fact, the `rgbif` package has already been used in more than 20 scholarly
publications (as of 2016-08-10), including [@Amano_2016, @Bartomeus_2013, @Barve_2014, @Bone_2015, @Collins_2015, @Drozd_2013, @Kong_2015, @Richardson_2015, @Turner_2015, @Verheijen_2015, @Zizka_2015, @Butterfield_2016, @Dellinger_2015, @Feitosa_2015, @Malhado_2015, @Werner_2015, @Robertson_2016, @Davison_2015, @Janssens_2016].

The `rgbif` package is relatively stable, and should not have
many breaking changes unless necessitated due to changes in the
GBIF API. However, it will gain function(s) to work with the maps
API in the near future.

The `pygbif` and `gbifrb` libraries are in early development, and will
greatly benefit from any feedback and use cases.

One area of focus in the future is to attempt to solve many use
cases that have been brought up with respect to GBIF data. For
example, some specimens are included in GBIF that are located
in botanical gardens. For many research questions, researchers are
interested in "wild" type occurrences, not those in human curated
scenarios. Making removal of these occurrences easy would be
very useful, but is actually quite a hard problem. There are many
other problems like this, for which these three libraries will help
in making more efficient and reproducible.

# Acknowledgments

This project was supported in part by the Alfred P Sloan Foundation
(Grant No. G-2014-13485), and in part by the Helmsley Foundation
(Grant No. 2016PG-BRI004).

# Data Accessibility

All scripts and data used in this paper can be found in the permanent
data archive Zenodo under the digital object identifier
(<https://doi.org/10.5281/zenodo.997554>). This DOI
corresponds to a snapshot of the GitHub repository at
<https://github.com/sckott/gbifms> that matches this preprint. Software
can be found at <https://github.com/ropensci/rgbif>, <https://github.com/sckott/pygbif>,
and <https://github.com/sckott/gibfrb>, all under MIT licenses. We thank
all the users that have used `rgbif`, `pygbif`, and `gbifrb` and have
given feedback and reported bugs. In addition, we greatly appreciate
all the contributors to the three libraries, found at
<https://github.com/ropensci/rgbif/graphs/contributors>,
<https://github.com/sckott/pygbif/graphs/contributors>, and
<https://github.com/sckott/gbifrb/graphs/contributors>.

# References

[mit]: http://choosealicense.com/licenses/mit/
[ghrgbif]: https://github.com/ropensci/rgbif
[cranrgbif]: https://cran.rstudio.com/web/packages/rgbif/
[gbifdocs]: http://www.gbif.org/developer/summary
[gbifissues]: http://gbif.github.io/gbif-api/apidocs/org/gbif/api/vocabulary/OccurrenceIssue.html
[taxize]: https://github.com/ropensci/taxize
[spocc]: https://github.com/ropensci/spocc
[mapr]: https://github.com/ropensci/mapr
[ghpygbif]: https://github.com/sckott/pygbif
[pypipygbif]: https://pypi.python.org/pypi/pygbif
