---
layout: review, 11pt
linenumbers: true
title: "rgbif: R client for working with GBIF species occurrence data"
author:
  - name: Scott Chamberlain
    affiliation: cstar
    email: scott(at)ropensci.org
    footnote: Corresponding author
address:
  - code: cstar
    address: |
      University of California, Berkeley, CA, USA
abstract: |
      1. xxx

      2. xxx

      3. xxx

      4. xxxx

bibliography: components/references.bib
csl: components/peerj.csl
documentclass: components/elsarticle

output:
  pdf_document:
    template: components/elsarticle.latex
    keep_tex: true
    fig_caption: true
---


```{r compile-settings, include=FALSE}
library("methods")
library("knitr")
opts_chunk$set(
  tidy = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = 1,
  comment = '#>',
  collapse = TRUE,
  verbose = TRUE
)
basename <- gsub(".Rmd", "", knitr:::knit_concord$get('infile'))
opts_chunk$set(fig.path = paste("components/figure/", basename, "-", sep=""),
               cache.path = paste("components/cache/", basename, "/", sep=""))
```

# Introduction

Perhaps the most fundamental element in many fields of ecology is
the individual. The number of individuals of each species in a given
location forms the basis for many sub-fields of ecology and evolution.
Some research questions necessitate collecting new data, while
others can easily take advantage of existing data. In fact, some
ecology fields are built largely on existing data, e.g., macro-ecology
[@Brown_1995; @Beck_2012].

Data on individuals, including which species, and where they're found,
can be used for a large number of research questions. Biodiversity
records have been used for a suite of other use cases:
validating habitat suitability models with real occurrence data
[@Ficetola_2014]; ancestral range reconstruction [@Ferretti_2015;
@Mendoza_2015]; development of invasive species watch lists
[@Faulkner_2014]; evaluating risk of invasive species spread
[@Di_Febbraro_2013]; and effects of climate change on future
biodiversity [@Brown_2015].

In addition to wide utility, this data is important for conservation.
Biodiversity loss is one of the greatest challenges of our time
[@Pimm_2014], and some have called this the sixth great mass
extinction [@Ceballos_2015]. Given this challenge there is a great
need for data on specimen records, whether collected from live sightings
in the field or specimens in museums.

There are many online services that collect and maintain specimen records.
However, Global Biodiversity Information Facility (hereafter, GBIF,
http://www.gbif.org) is the largest collection of biodiversity
records globally, currently with 640 million records, 1.6 million
taxa, 15,000 datasets from 780 publishers (current as of 2015-11-25).
Many large biodiversity warehouses such as iNaturalist
(http://www.inaturalist.org), VertNet (http://vertnet.org), and USGS's
Biodiversity Information Serving Our Nation (BISON; http://bison.usgs.ornl.gov)
all feed into GBIF.

Herein, we describe the `rgbif` software library [@rgbif] for working with
GBIF data in the R programming environment [@R]. R is a widely used
language in academia, and in non-profit and private sectors.
Importantly, R makes it easy to do all of the steps of the
research process, including data management, data manipulation
and cleaning, statistics, and vizualization. Thus, an R client
for getting GBIF data is a powerful tool to facilitate
reproducible research.

```{r echo=FALSE, results='hide', eval=FALSE}
install.packages("rgbif", dependencies=TRUE)
```

```{r echo=FALSE, results='hide'}
library("rgbif")
```

# The rgbif package

The `rgbif` package is nearly completely written in R (a small Javascript
library is included for reading well known text), uses an [MIT license][mit]
to maximize use everywhere. `rgbif` is developed publicly on GitHub
at [https://github.com/ropensci/rgbif][ghrgbif], where development versions
of the package can be installed, and bugs and feature requests reported.
Stable versions of `rgbif` can be installed from [CRAN][cranrgbif], the
distribution network for R packages. `rgbif` is part of the rOpenSci project
(http://ropensci.org), a developer network making R software to facilitate
reproducible research.

## Package interface

`rgbif` is designed following the [GBIF Application Programming Interface][gbifdocs],
or API. The GBIF API has four major components: registry, taxonomic names,
occurrences, and maps. We also include functions to interface with the OAI-PMH
GBIF service; only dataset information is available vis this service, however. We
ignore maps in `rgbif` as it is concerned with generating maps primarily for web
applications. `rgbif` has a suite of functions dealing with each of registry, taxonomic
names, and occurrences - we'll go through each in turn describing design and example usage.

## GBIF feedback loop

With each request `rgbif` makes to GBIF's API, we send request headers that
tell GBIF that the request is coming from `rgbif`, including what version of
the package. This helps GBIF know what proportion of requests are coming from
this package, and therefore R, as most requests likely will come from `rgbif`;
this information is helpful for them in thinking about how people are using
GBIF data.

## Registry

The GBIF reqistry API services are spread across five sets of functions
via the main API:

* Datasets
* Installations
* Networks
* Nodes
* Organizations

And dataset information in general is available via the OAI-PMH service,
functions in `rgbif` prefixed with `gbif_oai_`.

Datasets are owned by organizations. Organizations are endorsed by nodes to share
datasets with GBIF. Datasets are published through institutions, which may be
hosted at another organization. A network is a group of datasets (managed by GBIF).
Datasets are the units that matter the most with respect to registry information,
while installations, networks, nodes, and organizations are simply higher level
organizational structure.

### Datasets

Dataset functions include search, dataset metadata retrieval, and dataset metrics.
Searching for datasets is an important part of the discovery process. One can
search for datasets on the GBIF web portal. However, programmatic searching using
this package is much more powerful. Identifying datasets appropriate for a
research question is helpful as you can get metadata for each dataset, and track
down dataset specific problems, if any.

The `dataset_search()` function is one way to search for datasets. Here, we search
for the query term "oregon", which finds any datasets that have terms matching
that term.

```{r}
res <- dataset_search(query = "oregon")
res$data$datasetTitle[1:10]
```

See also `datasets()` and `dataset_suggest()` for searching for datasets.

#### Dataset metrics

Dataset metrics are another useful way of figuring out what
datasets you may want to use. One drawback is that these metrics data are only
available for datasets of type _checklist_, but there are quite a lot of
them (`r dataset_search(type = "checklist")$meta$count`).

Here, we seaerch for dataset metrics for a single dataset, with uuid
`ec93a739-1681-4b04-b62f-3a687127a17f`, a checklist of the ants (Hymenoptera:
Formicidae) of the World.

```{r eval=FALSE}
res <- dataset_metrics(uuid='ec93a739-1681-4b04-b62f-3a687127a17f')
data.frame(rank = names(res$countByRank),
           count = unname(unlist(res$countByRank)))
```

```{r echo=FALSE}
res <- dataset_metrics(uuid='ec93a739-1681-4b04-b62f-3a687127a17f')
df <- data.frame(rank = names(res$countByRank),
                 count = unname(unlist(res$countByRank)))
knitr::kable(df)
```

### Networks, nodes, and installations

Networks, nodes and installations are at a higher level of organization
above datasets, but can be useful if you want to explore data from given
organizations. Here, we search for the first 10 GBIF networks, returning
just the title field.

```{r}
networks(limit=10)$data$title
```

## Taxonomic names

The GBIF taxonomic names API services are spread across five functions:

* Search GBIF name backbone - `name_backbone()`
* Search across all checklists - `name_lookup()`
* Quick name lookup - `name_suggest()`
* Name usage of a name according to a checklist - `name_usage()`
* GBIF name parser - `parsenames()`

The goal of these name functions is often to settle on a taxonomic name
known to GBIF's database. This serves two purposes: 1) when referring to
a taxonomic name, you can point to a URI on the internet, and 2) you can
search for metadata on a taxon, and occurrences of that taxon in
GBIF.

Taxonomic names are particularly tricky. Many different organizations have
their own unique codes for the same taxonomic names, and some taxonomic
groups have preferred sources for the definitive names for that group. That's
why it's best to determine what name GBIF uses, and its associated identifier,
for the taxon of interest instead of simply searching for occurrences with a
taxonomic name.

When searching for occurrences (see below) you can search by taxonomic
name (and other filters, e.g., taxonomic rank), but you're probably better off
figuring out the taxonomic key in the GBIF backbone taxonomy, and using
that to search for occurrences. The `taxonkey` parameter in the GBIF occurrences
API expects a GBIF backbone taxon key.

### GBIF Backbone

The GBIF backbone taxonomy is used in GBIF to have a consistent way to refer
to taxonomic names throughout their services. The backbone has `r dataset_metrics("d7dddbf4-2cf0-4f39-9b2a-bb099caae36c")$distinctNamesCount` unique
names and `r dataset_metrics("d7dddbf4-2cf0-4f39-9b2a-bb099caae36c")$countByRank$SPECIES` species names. The backbone taxonomy is also a dataset with key
`d7dddbf4-2cf0-4f39-9b2a-bb099caae36c`
(http://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c).

We can search the backbone taxonomy with the function `name_backbone()`. Here,
we're searching for the name _Poa_, restricting to genera, and the family
_Poaceae_.


```{r}
res <- name_backbone(name='Poa', rank='genus', family='Poaceae')
res[c('usageKey', 'kingdom')]
```

### Name searching

One of the quickest ways to search for names is using `name_suggest()`, which
does a very quick search and returns minimal data. Here, we're searching for
the query tem _Pum_, and we get back many names:

```{r eval=FALSE}
name_suggest(q='Pum', limit = 6)
```

```{r echo=FALSE}
df <- name_suggest(q='Pum', limit = 6)
knitr::kable(df)
```

With these results, you can then proceed to search for occurrences with the
taxon key(s), or drill down further with other name searching functions to
get the exact taxon of interest.

## Occurrences

GBIF provides two ways to get occurrence data: through the
`/occurrence/search` route (see `occ_search`), or via the
`/occurrence/download` route (many functions, see below).
`occ_search()` is the main funtion for the search route,
and is more appropriate for smaller data, while
`occ_download*()` functions are more appropriate for larger
data requests.

Large is of course a subjective term. When you hit a "large dataset"
will depend primarily on the size of the your data request. GBIF imposes
for any given search a limit of 200,000 records in the search service,
after which point you can't download any more records for that search.
However, you can download more records for different searches.

We think the search service is still quite useful for many people
even given the 200,000 limit. For those that need more data, we have
created a similar interface in the `download_*()` functions, that should
be easy to use. Users should take note that using the download service
has a few extra steps to get data into R, but is straight-forward.

The download service, like the occurrence search service, is rate-limited.
That is, you can only have one to three downloads running simultaneously for
your user credentials. However, simply check when a download job is complete,
then you should be able to start a new download request.

### Download API

The download API syntax is similar to the occurrence search API
in that the same parameters are used, but the way in which the query
is defined is different. For example, in the download API you can
do greater than searches (i.e., `latitude > 50`), whereas you can not
do that in the occurrence search API. Thus, unfortnately, we couldn't
make the query interace exactly the same for both search and
download functions.

Using the download service can consist of as few as three steps: 1) Request
data via a search; 2) Download data; 3) Import data into R.

Request data download given a query. Here, we search for the taxon key
`3119195`, which is the key for _Helianthus annuus_
(http://www.gbif.org/species/3119195).

```{r eval = FALSE}
occ_download('taxonKey = 3119195')
#> <<gbif download>>
#>   Username: xxxx
#>   E-mail: xxxx
#>   Download key: 0000840-150615163101818
```

You can check on when the download is ready using the functions
`occ_download_list()` and `occ_download_meta()`. When it's ready
use `occ_download_get()` to download the dataset to your computer.

```{r}
(res <- occ_download_get("0000840-150615163101818", overwrite = TRUE))
```

What's printed out above is a very brief summary of what was downloaded,
the path to the file, and its size (in human readable form).

Next, read the data in to R using the function `occ_download_import()`.

```{r}
library("dplyr")
dat <- occ_download_import(res)
dat %>%
  select(gbifID, decimalLatitude, decimalLongitude)
```

#### Downloaded data format

The downloaded dataset from GBIF is actually a Darwin Core Archive (DwC-A), an
internationally recognized biodiversity informatics standard
(http://rs.tdwg.org/dwc/). The DwC-A downloaded is a compressed folder with
a number of files, including metadata, citations for each of the datasets included
in the download, and the data itself, in separate files for each dataset as well
as one single `.txt` file. In `occ_download_import()`, we simply fetch data from the
`.txt` file. If you want to dig into the metadata, citations, etc., it is easily
accessible from the folder on your computer.

### Search API

The search API follows the GBIF API and is broken down into the following functions:

* Get a single numeric count of occurrenes - `occ_count()`
* Search for occurrences - `occ_search()`
* Get occurrences by occurrence identifier - `occ_get()`
* Get occurrence metadata - `occ_metadata()`


#### Search for occurrences

The main search work-horse is `occ_search()`. This function allows very flexible
search definitions. In addition, this function does paging internally, making it
such that the user does not have worry about the 300 records per request limit -
but of course we can't go over the 200,000 maximum limit.

The output of `occ_search()` borrows the tidy `data.frame` idea from the `dplyr`
R package, so that no matter how large the `data.frame`, the output is easily
assessed because only a few of the records (rows) are shown, only a few columns
are shown (with others shown in name only), and metadata is shown on top of
the `data.frame` to indicate data found and returned, media
records found, unique taxonomic hierarchies returned, and the query executed.

The output of these examples, except one, aren't shown, but all run correctly.

Search by species name, using `name_backbone()` first to get key

```{r}
(key <- name_suggest(q = 'Helianthus annuus', rank = 'species')$key[1])
occ_search(taxonKey = key, limit = 2)
```

Instead of getting a taxon key first, you can search for a name directly

```{r eval=FALSE}
occ_search(scientificName = 'Ursus americanus')
```

Search for many species

```{r eval=FALSE}
splist <- c('Cyanocitta stelleri', 'Junco hyemalis', 'Aix sponsa')
keys <- sapply(splist, function(x) name_suggest(x)$key[1], USE.NAMES = FALSE)
occ_search(taxonKey = keys, limit = 5, return = 'data')
```

Spatial search, based on well known text format, or a bounding box set of
four coordinates

```{r eval=FALSE}
# well known text
occ_search(geometry = 'POLYGON((30.1 10.1, 10 20, 20 40, 40 40, 30.1 10.1))', limit = 20)
# bounding box
occ_search(geometry = c(-125.0,38.4,-121.8,40.9), limit = 20)
```

Get only occurrences with lat/long data

```{r eval=FALSE}
occ_search(hasCoordinate = TRUE, limit = 5)
```

Get only those occurrences with spatial issues

```{r eval=FALSE}
occ_search(hasGeospatialIssue = TRUE, limit = 5)
```

#### Cleaning data

GBIF provides optional data issues with each occurrence record. These issues fall
into many different pre-defined classes, covering issues with taxonomic names,
geographic data, and more (see `occ_issues_lookup()` to find out more information
on GBIF issues; and the same data on [GBIF's development site][gbifissues]).

`occ_issues()` provides a way to easily filter data downloaded via `occ_search()`
based on GBIF issues.

```{r}
out <- occ_search(issue = 'DEPTH_UNLIKELY', limit = 500)
NROW(out)
out %>% occ_issues(-cudc) %>% .$data %>% NROW
```

# Use cases

### Ecological niche modelling

```{r niche-modelling, child="use-cases/niche-modelling.Rmd"}
```

### Biodiversity in big cities

```{r city-diversity, child="use-cases/city-diversity.Rmd"}
```

### Valley oak occurrence data comparison

```{r oak-occurrences, child="use-cases/oak-occurrences.Rmd"}
```

# Conclusions and future directions

The `rgbif` R package provides a programmatic interface to GBIF's
application programming interface (API) - a powerful tool for making
research using species occurrence data reproducible. In fact,
the `rgbif` package has already been used in 22 scholarly publications
(as of 2015-11-14).

The `rgbif` package is relatively stable now, and should not have
many breaking changes unless necessitated due to changes in the
GBIF API.

One area of focus in the future is to attempt to solve many use
cases that have been brought up with respect to GBIF data. For
example, some specimens are included in GBIF that are located
in botanical gardens. For many research questions, researchers are
interested in "wild" type occurrences, not those in human curated
scenarios. Making removal of these occurrences easy would be
very useful, but is actually quite a hard problem.

# Acknowledgements

This project was supported in part by the Alfred P Sloan
Foundation (Grant 2013-6-22).

# Data Accessibility

All scripts and data used in this paper can be found in the permanent
data archive Zenodo under the digital object identifier (DOI). This DOI
corresponds to a snapshot of the GitHub repository at
[github.com/sckott/msrgbif](https://github.com/sckott/msrgbif). Software
can be found at [github.com/ropensci/rgbif](https://github.com/ropensci/rgbif),
under the open and permissive MIT license.

# References

[mit]: http://choosealicense.com/licenses/mit/
[ghrgbif]: https://github.com/ropensci/rgbif
[cranrgbif]: https://cran.rstudio.com/web/packages/rgbif/
[gbifdocs]: http://www.gbif.org/developer/summary
[gbifissues]: http://gbif.github.io/gbif-api/apidocs/org/gbif/api/vocabulary/OccurrenceIssue.html
